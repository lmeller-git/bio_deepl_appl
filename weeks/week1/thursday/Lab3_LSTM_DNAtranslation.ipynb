{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8d781c1-022f-4af6-8ba3-4da21a9544cf",
   "metadata": {},
   "source": [
    "# Lab 3: RNNs and LSTMs\n",
    "In this tutorial we create neural networks using [Long Short-Term Memory (LSTM)](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) cells. The data and its pre-processing for of this notebook is identical to the first lab (Perceptron) to keep the amount of new information limited. Some of the required code-blocks are empty - requiring your imput to complete the model. A few additional questions at the end challenge you to play around with the code and try things for yourselves.\n",
    "\n",
    "During the session, you will create a LSTM to translate DNA sequences into protein sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c25dd79e-1645-44c5-84ce-c9b0f540de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import lightning as L\n",
    "\n",
    "# import basic functionality\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# libraries for plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "torch.set_default_dtype(torch.float16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814c75e-4c3e-4d74-bcca-6adfba516948",
   "metadata": {},
   "source": [
    "# Step 1: Pre-processing the data\n",
    "Here we download and pre-process the dataset. As before, we only consider DNA sequences that are protein coding, contain a integer number of codons, have a start and stop codon, and do not contain any uncertain nucleotides. Finally, we remove duplicates and randomly mix the sequences. Nothing is different from the last time, so you can simply execute these steps and move on to Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3fb6e8-f264-4584-b4d2-f8462239cc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/course\n",
      "--2025-02-20 16:44:29--  https://ftp.ensembl.org/pub/current_fasta/homo_sapiens/cds/Homo_sapiens.GRCh38.cds.all.fa.gz\n",
      "Resolving ftp.ensembl.org (ftp.ensembl.org)... 193.62.193.169\n",
      "connected. to ftp.ensembl.org (ftp.ensembl.org)|193.62.193.169|:443... \n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 23163561 (22M) [application/x-gzip]\n",
      "Saving to: ‘/home/course/all_seqs/Homo_sapiens.GRCh38.cds.all.fa.gz’\n",
      "\n",
      "Homo_sapiens.GRCh38 100%[===================>]  22.09M  30.4MB/s    in 0.7s    \n",
      "\n",
      "2025-02-20 16:44:30 (30.4 MB/s) - ‘/home/course/all_seqs/Homo_sapiens.GRCh38.cds.all.fa.gz’ saved [23163561/23163561]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# download and unpack DNA coding sequences for human, mouse and yeast\n",
    "############################\n",
    "\n",
    "!mkdir -p ~/all_seqs\n",
    "%cd ~/\n",
    "\n",
    "!wget -P ~/all_seqs/ https://ftp.ensembl.org/pub/current_fasta/homo_sapiens/cds/Homo_sapiens.GRCh38.cds.all.fa.gz\n",
    "!gzip -df \"all_seqs/Homo_sapiens.GRCh38.cds.all.fa.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350c0eeb-b2da-4ae8-a5cf-4149a018f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function that loads and processes a FASTA file containing coding sequences\n",
    "def load_species_cds(file_name):\n",
    "    dna_seq = []\n",
    "    prot_seq = []\n",
    "    for record in SeqIO.parse(file_name, \"fasta\"):\n",
    "        # ensure that sequences are protein coding\n",
    "        if 'gene_biotype:protein_coding' in record.description:\n",
    "            if 'transcript_biotype:protein_coding' in record.description:\n",
    "                if ' cds ' in record.description:\n",
    "                    if len(record.seq) % 3 == 0:\n",
    "                        dna_seq.append(str(record.seq))\n",
    "                        prot_seq.append(str(record.seq.translate()))\n",
    "                        \n",
    "    # keep sequences that are protein coding\n",
    "    dna_seq_cod = []\n",
    "    prot_seq_cod = []\n",
    "    for i in range(len(prot_seq)):\n",
    "        if (prot_seq[i][0]=='M') & (prot_seq[i][-1]=='*'):\n",
    "            dna_seq_cod.append(dna_seq[i])\n",
    "            prot_seq_cod.append(prot_seq[i])\n",
    "\n",
    "    # avoid sequences with undetermined/uncertain nucleotides\n",
    "    dna_seq_cod = [dna_seq_cod[i] for i in range(len(dna_seq_cod)) if ('N' not in dna_seq_cod[i])]\n",
    "    prot_seq_cod = [prot_seq_cod[i] for i in range(len(dna_seq_cod)) if ('N' not in dna_seq_cod[i])]\n",
    " \n",
    "    # remove duplicates and randomly mix the list of sequences\n",
    "    seqs = list(zip(dna_seq_cod, prot_seq_cod))\n",
    "    seqs = list(set(seqs))\n",
    "    random.shuffle(seqs)\n",
    "    dna_seq_cod, prot_seq_cod = zip(*seqs)\n",
    "\n",
    "    # pack samples as a list of dictionaries and return result\n",
    "    seq_data = [{'dna':dna_seq_cod[i],'prot':prot_seq_cod[i]} for i in range(len(dna_seq_cod))]\n",
    "    return seq_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e4ce519-1289-42de-9007-7a1690110b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading human proteins\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ATGGACACGTTCAGCACCGTCATCTCCAGCAGTATTCAGC...',\n",
       " 'ATGGCCCAACCAGAGAGTTCTGCAGATTTTGGCATAGCTC...',\n",
       " 'ATGGAAATAGAAGAGGAGTCTGCTGAAAAGATTCAATTTC...',\n",
       " 'ATGGACATATCAATACAAAGAAATGGAAGCATCGTTGCTG...',\n",
       " 'ATGGAGTCCTACGACATCATCGCCAACCAGCCTGTGGTCA...']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# load coding sequences for different species\n",
    "print('loading human proteins')\n",
    "seq_data = load_species_cds(\"all_seqs/Homo_sapiens.GRCh38.cds.all.fa\")\n",
    "\n",
    "# take a look at some sequences\n",
    "[seq_data[i]['dna'][0:40]+'...' for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "783a350a-500e-43fd-8bd5-2becb21d0965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences:  53095\n"
     ]
    }
   ],
   "source": [
    "print('number of sequences: ', len(seq_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ce85a-62cb-484c-a4d2-4acdb208b4b2",
   "metadata": {},
   "source": [
    "# Step 2: Encoding the sequences\n",
    "Having prepared the coding sequences and their translation, we convert them into a numeric representation as vectors. To do so, we first construct a language class that stores words of each language and allows us to convert between encoding/indices and words in a language. We define a function that allows us to store any sequence of words (i.e., codons or bases) as a numeric representation. Here we extend every sequence with a start of sentence <SOS> and end of sentence <EOS> token, such that the model knows when to start and stop translating. Finally, we can extend every sentence to the same length by padding with an empty \"word\" that is not translated or used, but allows us to use the identical-length numerical representation of the sentences as input for the model. The language allows for a simple encoding of words as numbers or as numerical vectors (one-hot-encoding). The 'encode' function converts an input sentence to the specified encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050adab-da35-4577-a3bb-3b336caf1a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class to store a language\n",
    "class Language:\n",
    "    # initialize the language, as standard we have start of sentence (SOS), end of sentence (EOS) and a padding to equalize sentence lengths (PAD)\n",
    "    def __init__(self, name, codon_len):\n",
    "        self.name = name\n",
    "        self.word2index = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2}\n",
    "        self.encoding = {}\n",
    "        self.index2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\"}\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "        self.codon_length = codon_len\n",
    "        self.max_len = 0\n",
    "\n",
    "    # function to add sentence to language (add new words in the sentence to our language)\n",
    "    def addSentence(self, sentence):\n",
    "        if len(sentence) > self.max_len:\n",
    "            self.max_len = len(sentence)\n",
    "        for word in [sentence[i:i+self.codon_length] for i in range(0, len(sentence), self.codon_length)]:\n",
    "            self.addWord(word)\n",
    "\n",
    "    # function to add word to language\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "            \n",
    "    # function to convert indices to one-hot encodings (i.e., 3 becomes [0,0,0,1,0,0,...])\n",
    "    def as_one_hot(self):\n",
    "        for key in self.word2index:\n",
    "            new_val = np.zeros(len(self.word2index),dtype=np.int32)\n",
    "            new_val[self.word2index[key]] = 1\n",
    "            self.encoding[key] = new_val\n",
    "    \n",
    "    # function to convert indices to simple encodings\n",
    "    def as_encoding(self):\n",
    "        self.encoding = self.word2index\n",
    "\n",
    "    # function to encode (and pad) a sentence\n",
    "    # we use this to take an input sentence and convert it to a sequence of arrays that represent that sentence in a given language\n",
    "    # in the context of proteins, think of this as encoding the bases or codons\n",
    "    def encode(self, sentence, max_len=None):\n",
    "        sos = [self.encoding[\"<SOS>\"]]\n",
    "        eos = [self.encoding[\"<EOS>\"]]\n",
    "        pad = [self.encoding[\"<PAD>\"]]\n",
    "\n",
    "        # split sentence in blocks of a given codon_length\n",
    "        sentence_split = [sentence[i:i+self.codon_length] for i in range(0, len(sentence), self.codon_length)]\n",
    "            \n",
    "        # encode sentence in the given language\n",
    "        sentence_encoded = [self.encoding[word] for word in sentence_split]\n",
    "        max_len = round(max_len if max_len is not None else self.max_len / self.codon_length)\n",
    "        # only pad or truncate if a maximum length is specified\n",
    "        if max_len is not None:\n",
    "            if len(sentence_split) < max_len - 2: \n",
    "                # sentence is shorter than max length-2; add SOS and EOS and pad to maximum length\n",
    "                n_pads = max_len - 2 - len(sentence_split)\n",
    "                return torch.Tensor(np.array(sos + sentence_encoded + eos + pad * n_pads, dtype = np.float16))\n",
    "            else: \n",
    "                # sentence is longer than max length; truncate and add SOS and EOS\n",
    "                sentence_truncated = sentence_encoded[:max_len - 2]\n",
    "                return torch.Tensor(np.array(sos + sentence_truncated + eos, dtype = np.float16))\n",
    "        else:\n",
    "            return torch.Tensor(np.array(sos + sentence_encoded + eos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step2a: create languages for DNA and protein sequences\n",
    "########################\n",
    "\n",
    "# create a language for DNA and protein sequences\n",
    "dna_lang = Language(name=\"dna\", codon_len=3)\n",
    "prot_lang = Language(name=\"prot\", codon_len=1)\n",
    "\n",
    "# split the sequence data ('seq_data') that we defined above into sensible training, validation and test sets\n",
    "# think about how much data would realistically be necessary to learn the problem of translating DNA sequences\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(seq_data, [0.7, 0.15, 0.15])\n",
    "\n",
    "# memorize the dna and protein languages by parsing all sequences\n",
    "for cur_seq in train_set:\n",
    "    dna_lang.addSentence(cur_seq['dna'])\n",
    "    prot_lang.addSentence(cur_seq['prot'])\n",
    "\n",
    "# create an one-hot-encoding for all words codons and a simple encoding for all amino acids\n",
    "# call the appropriate functions for each of the two languages\n",
    "dna_lang.as_one_hot()\n",
    "prot_lang.as_one_hot()\n",
    "\n",
    "print(dna_lang.max_len)\n",
    "print(prot_lang.max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be9f436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4eeb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# here we define a function for encoding a dataset of dna and protein sequences\n",
    "def encode_dataset(dataset, dna_lang, prot_lang, max_length):\n",
    "    dataset_encoded = [\n",
    "        { \n",
    "          'dna'  : dna_lang.encode(dataset[i]['dna'], max_len=max_length),\n",
    "          'prot' : prot_lang.encode(dataset[i]['prot'], max_len=max_length)\n",
    "        } for i in range(len(dataset))\n",
    "    ]\n",
    "    return dataset_encoded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d24eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step2b: encode your sequences here\n",
    "########################\n",
    "\n",
    "# define maximum number of codons\n",
    "# we truncate any sequence longer than this length, and pad any sequence shorter than this length\n",
    "# think about a sensible length for the input sequences\n",
    "max_length = None\n",
    "\n",
    "# encode the training and validation data\n",
    "train_set_encoded = encode_dataset(train_set, dna_lang, prot_lang, max_length)\n",
    "val_set_encoded = encode_dataset(val_set, dna_lang, prot_lang, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b7fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c90e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set_encoded[0]['dna'].shape)\n",
    "# take a look at the encoding of a DNA\n",
    "train_set[0]['dna'], train_set_encoded[0]['dna']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set_encoded[0]['prot'].shape)\n",
    "# take a look at the encoding of a protein\n",
    "train_set[0]['prot'], train_set_encoded[0]['prot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c56393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define dataloader for the encoded sequences\n",
    "def get_dataloader(dataset, batch_size):\n",
    "    cur_sampler = RandomSampler(dataset)\n",
    "    cur_dataloader = DataLoader(dataset, sampler=cur_sampler, batch_size=batch_size, drop_last=True, num_workers=15)\n",
    "    return cur_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03de687-77f7-442c-8e89-75039b83137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step 2c: create a dataloader for the validation and training sequences\n",
    "########################\n",
    "\n",
    "# how many samples should be trained on simultaneously?\n",
    "batch_size = 20\n",
    "\n",
    "# define dataloader for training\n",
    "train_loader = get_dataloader(train_set_encoded, batch_size)\n",
    "val_loader = get_dataloader(val_set_encoded, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ecc13-1767-4997-a7f0-b295a5e8a413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09d563e8-254f-495e-9a96-ffbfc60e54c3",
   "metadata": {},
   "source": [
    "# Step 3: Define model\n",
    "We created languages for DNA and protein sequences, and encoded all sequences through the encodings defined by these languages. We then created data loaders for these encoded sequences. As a final preparation, we define our [model](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). \n",
    "Create a class that instructs pytorch to make a LSTM model using the given input, hidden size and output parameters. You'll have to define the init and forward functions. Additionally, in the Pytorch Lightning class, you must choose a loss function and optimizer that are appropriate for the problem you are trying to solve. Once you have thought about your model, we will go over the architecture together with the class. Hint: for the LSTM, you'll need to also define the hidden and cell states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dacb684-577f-4acc-91c0-11ccf6decd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c12bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step 3a: define the model architecture\n",
    "########################\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyLSTM,self).__init__()\n",
    "\n",
    "        self.l1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.act =  nn.Identity()#nn.ReLU()\n",
    "        self.l2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.l3 = nn.LSTM(hidden_size, output_size, batch_first=True)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        #print(inp.shape)\n",
    "        inp.to(device)\n",
    "        output, (hn2, cn2) = self.l2(self.act(output))\n",
    "        output, (hn3, cn3) = self.l3(self.act(output))\n",
    "        return output #nn.Softmax()(output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce47f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c898b30-16e4-45ff-9bc3-a5d61253b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step 3b: define the lightning module to train the model\n",
    "########################\n",
    "\n",
    "# lightning module to train the sequence model\n",
    "class SequenceModelLightning(L.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.1):\n",
    "        super().__init__()\n",
    "        self.model = MyLSTM(input_size, hidden_size, output_size).double()\n",
    "        self.lr = lr\n",
    "        # define loss function here, pseudocode:\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_tensor = batch[\"dna\"].double()\n",
    "        target_tensor = batch[\"prot\"].double()\n",
    "\n",
    "        output = self.model(input_tensor)\n",
    "        loss = self.loss(output,target_tensor)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_tensor = batch[\"dna\"].double()\n",
    "        target_tensor = batch[\"prot\"].double()\n",
    "\n",
    "        output = self.model(input_tensor)\n",
    "        #print(output.shape, target_tensor.shape)\n",
    "        #print(output.view(-1, output.shape[2]).shape, target_tensor.view(-1).long().shape)\n",
    "        loss = self.loss(output,target_tensor)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # define optimizer here\n",
    "        return torch.optim.Adam(self.model.parameters(), self.lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e930bbd3-ba4b-4963-915f-8d494ff7870d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a47bdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traininig\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model | MyLSTM           | 329 K  | train\n",
      "1 | loss  | CrossEntropyLoss | 0      | train\n",
      "---------------------------------------------------\n",
      "329 K     Trainable params\n",
      "0         Non-trainable params\n",
      "329 K     Total params\n",
      "1.318     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa00382996eb48519bd4c238f7205399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/course/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/course/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/course/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/course/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/course/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 127, in collate\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/course/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 127, in <dictcomp>\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/course/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/course/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [310, 67] at entry 0 and [646, 67] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# learn the weights of the model\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraininig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(lit_model, train_loader, val_loader)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    545\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1028\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1028\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1030\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1057\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1057\u001b[0m val_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1059\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:128\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     batch, batch_idx, dataloader_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous_dataloader_idx \u001b[38;5;241m!=\u001b[39m dataloader_idx:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# the dataloader has changed, notify the logger connector\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__next__\u001b[39m()\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_profiler()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ITERATOR_RETURN:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py:142\u001b[0m, in \u001b[0;36m_Sequential.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_next_iterator()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/course/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/course/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/course/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/course/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 127, in collate\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/course/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 127, in <dictcomp>\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/course/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/course/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [310, 67] at entry 0 and [646, 67] at entry 1\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "### step3c: define the input parameters for the training loop\n",
    "########################\n",
    "\n",
    "# define the model and training loop\n",
    "# think of the dimensionality of your input data (dna sequences) and output data (protein sequence), and where these numbers are stored\n",
    "lit_model = SequenceModelLightning(input_size = 67,\n",
    "                                  hidden_size = 150,\n",
    "                                  output_size = 24,\n",
    "                                  lr = 8e-3)\n",
    "\n",
    "# define the trainer\n",
    "trainer = L.Trainer(devices = 1, \n",
    "                    max_epochs = 15)\n",
    "\n",
    "# learn the weights of the model\n",
    "print(\"traininig\")\n",
    "trainer.fit(lit_model, train_loader, val_loader)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590813ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4678abab-c139-4349-98c3-8fdbae185664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the performance of your model on the validation data\n",
    "trainer.validate(lit_model, val_loader)\n",
    "\n",
    "# show the model architecture\n",
    "my_lstm = lit_model.model\n",
    "my_lstm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6ef5b-8de7-400a-b7bd-e62130182ec7",
   "metadata": {},
   "source": [
    "# Step 4: Test the model on random test sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "05c98e35-de82-402d-a555-e6528908ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we encode the test data using the same dna and protein language encodings we defined before\n",
    "# if you change the languages, you need to re-encode the test sequences as well!\n",
    "test_set_encoded = encode_dataset(test_set, dna_lang, prot_lang, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d47a6dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 24])\n",
      "68\n",
      "     MDAPARLLAPLLLLCAQQLRGTRAMNDIGDYVGSNLEISWLPNLDGLIAGYARNFRPGIGGPPVNVALALEVASIDHISEANMEYTMTVFLHQSWRDSRLSYNHTNETLGLDSRFVDKLWLPDTFIVNAKSAWFHDVTVENKLIRLQPDGVILYSIRITSTVACDMDLAKYPMDEQECMLDLESYGYSSEDIVYYWSESQEHIHGLDKLQLAQFTITSYRFTTELMNFKSAGQFPRLSLHFHLRRNRGVYIIQSYMPSVLLVAMSWVSFWISQAAVPARVSLGITTVLTMTTLMVSARSSLPRASAIKALDVYFWICYVFVFAALVEYAFAHFNADYRKKQKAKVKVSRPRAEMDVRNAIVLFSLSAAGVTQELAISRRQRRVPGNLMGSYRSVGVETGETKKEGAARSGGQGGIRARLRPIDADTIDIYARAVFPAAFAAVNVIYWAAYAM*\n",
      "<SOS>MDAPARLLAPLLLLCAQQLRGTRAMNDIGDYVGSNLEISVLPNLDGLIAGYARNFRPGIGGPV\n",
      "\n",
      "Accuracy of aa calling over the sequence:  0.9682539682539683\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "### step4: define the input tensor and get the prediction from your model\n",
    "########################\n",
    "\n",
    "# pick a random sequence from the test set\n",
    "random_pair = np.random.randint(0,len(test_set))\n",
    "\n",
    "# get the encoded dna sequence and its known protein translation\n",
    "dna_sequence = np.array([test_set_encoded[random_pair]['dna']])\n",
    "protein_translation = test_set[random_pair]['prot']\n",
    "\n",
    "# send model and input sequence to device, compute translation of sequence\n",
    "my_lstm.cuda()\n",
    "input_tensor = torch.from_numpy(dna_sequence).to(device).double()\n",
    "output = my_lstm(input_tensor)\n",
    "print(output.shape)\n",
    "# convert output back to protein sequence by taking the most likely amino acid per position, print results\n",
    "result = \"\".join([prot_lang.index2word[i] for i in output.cpu().topk(1)[1].view(-1).numpy()])\n",
    "\n",
    "print(len(result))\n",
    "print('     '+protein_translation)\n",
    "print(result, end='\\n\\n')\n",
    "\n",
    "# print accuracy\n",
    "result = \"\".join([prot_lang.index2word[i] for i in output.cpu().topk(1)[1].view(-1).numpy() if i not in [key for key in Language('',1).index2word]])\n",
    "min_len = np.min([len(result),len(protein_translation)])\n",
    "print('Accuracy of aa calling over the sequence: ', np.sum([protein_translation[i] == result[i] for i in range(min_len)])/min_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7503d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0981196-e4ed-43bf-a98f-5cc29905f974",
   "metadata": {},
   "source": [
    "# Steps: <br>\n",
    "2a: create languages for the DNA and protein sequences <br>\n",
    "2b: encode the training and validation sequences <br>\n",
    "2c: create a dataloader for the validation and training sequences <br>\n",
    "3a: define your LSTM model <br>\n",
    "3b: define the lightning module to train the model <br>\n",
    "3c: define the input parameters for the training loop <br>\n",
    "Train your model :) <br>\n",
    "4: define the input tensor and get the prediction from your model <br>\n",
    " <br>\n",
    "# Questions: <br>\n",
    "-The test sequences are truncated at or padded to max_length. Change the encoding of the test sequences to work for arbitrary lengths (the actual length of the sequences). <br>\n",
    "-The model is trained on truncated and padded sequences. Change the setup to train your model on arbitrary length sequences (their actual length). Before you train the model, think about a) the number of samples to use for training, b) batch sizes, c) number of epochs <br>\n",
    "-Can you modify the code to train it on a single DNA sequence (i.e. a single sample)? Can you achieve perfect/reasonable accuracy? <br>\n",
    "-What is the minimum model size to reach a 'good' accuracy? What is the minimum number of required samples? Think about these questions in context of a classical machine learning classifier <br>\n",
    "-Can you change the code to use a DNA language for single nucleotides instead of codons and train the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a7c7b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
