# Embeddings in protein language models  
**Q1.** Have a look at the description of the ESM-2 model architecture (Section A.2.3), and Table S3 in the supplementaries of ESM-2 paper ([Lin et al., 2023](https://doi.org/10.1126/science.ade2574)). How many parameters would you expect to see in `esm_model`? Can you adapt the code below to (approximately) count all the parameters?  
**Q2.** Can you interpret the output of the code below? Can you adapt the code to calculate ESM-2 scores around residue D205 of the GCK glucokinase, and compare the results obtained using ESM-2 to what was observed with AlphaMissense on [Figure 3F](https://www.science.org/doi/10.1126/science.adg7492#sec-5) of [Lin et al., 2023](https://doi.org/10.1126/science.ade2574)?  
**Q3.** Adapt the code below to visualise alpha helices, beta sheets, and binding sites on the GCK glucokinase structure.  
**Q4.** The code below runs the raw ESM-2 model on the unmasked sequence, capturing hidden state. Find and describe what is returned as hidden state in `outputs`. Complete the function `get_layer_from_output()` and `get_embeddings_from_output()` to extract a specific embedding from a specific layer.  
**Q5.** Have a look at Figures 4 and 5 in [Vig et al., 2021](https://doi.org/10.48550/arXiv.2006.15222) describing how different layers of protein language models seem better at capturing different features on the protein. Try to understand the code below, and adapt it to beta sheets and binding sites. Plot the results in the next cell (complete the code as necessary), and compare the results to what is described in the paper. Finally, use the visualisation code above to plot the best predictor embeddings for alpha helices, beta sheets and binding sites.  
